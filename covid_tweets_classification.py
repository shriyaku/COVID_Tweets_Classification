# -*- coding: utf-8 -*-
"""covid_tweets_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HgZx7rxCS2_w8u1wIyKWwkv23qwigO0Q

# Training a classifier to detect tweets that self-report COVID-19 diagnosis

# Install Flair.
"""

!pip install flair -qq

"""# Import the modules."""

import os
from os import mkdir, listdir
from os.path import join, exists
import re

from torch.optim.adam import Adam
from flair.datasets import CSVClassificationCorpus
from flair.data import Corpus, Sentence
from flair.embeddings import TransformerDocumentEmbeddings
from flair.models import TextClassifier
from flair.trainers import ModelTrainer

"""# Connect to the data directory in Google Drive.
1.   Click on the "folder" icon in the left panel (below the "x" icon).
2.   In the "Files" pop-up window, click on "Drive" icon (to the left of the "eye" icon). 
3. Run the cell to mount your Google Drive.
4. Click "Connect to Google Drive".
5. Select your Google account.
6. Click "Allow".
7. Run the cell below, which will load the `covid/` directory.
"""

BASE_DIR = '/content/drive/My Drive/colab_data/covid/'
os.chdir(BASE_DIR)
print("Main directory:", listdir())
print("Data directory:", listdir("data"))

from google.colab import drive
drive.mount('/content/drive')

"""# Preview the first 10 lines of the training and test sets.
 
"""

!wc -l data/*tsv ; echo
!head data/covid_diagnosis_tweets_training.tsv ; echo
# !head data/covid_diagnosis_tweets_development.tsv ; echo
!head data/covid_diagnosis_tweets_test.tsv ; echo

"""# Define the preprocessing.
In this case, we will normalize usernames and URLs in the tweets, and lowercase the text.
"""

# regular expressions to match usernames and URLs
twitter_username_re = re.compile(r'@([A-Za-z0-9_]+)')
twitter_url_re = re.compile(r'https?:\/\/\S+')

# normalize usernames and URLs and lowercase the text
def preprocess(text, lower=True):
  text = re.sub(twitter_username_re, "@USER", text)
  text = re.sub(twitter_url_re, "HTTPURL", text)
  text = text.lower() if lower else text
  text = text.replace("\"", " ")
  return text

"""# Apply the preprocessing to the training, development, and test sets.
In this case, we will automatically generate a development set by creating a 5% split of the training set. The preprocessed training ("train.tsv"), development ("dev.tsv"), and test ("test.tsv") sets will be stored in a subdirectory of `covid/` called `resources/processed/`.  
"""

# preprocess the training and developement sets
def preprocess_training(in_file, out_dir):
  count = 0
  trainfile = open(join(out_dir, "train.tsv"), "w")
  devfile = open(join(out_dir, "dev.tsv"), "w")
  for line in open(in_file):
    count += 1
    parts = line.strip().split("\t")
    # skip the header
    if parts[0] == "tweet_id":
        print("\t".join(parts), file=trainfile)
        print("\t".join(parts), file=devfile)
        continue
    proc_tweet = preprocess(parts[2])
    if count % 20 == 0:
        print("\t".join([parts[0], parts[1], proc_tweet, parts[3], parts[4]]), file=devfile)
    else:
        print("\t".join([parts[0], parts[1], proc_tweet, parts[3], parts[4]]), file=trainfile)
  trainfile.close()
  devfile.close()

# # preprocess the training set
# def preprocess_training(in_file, out_dir):
#   count = 0
#   trainfile = open(join(out_dir, "train.tsv"), "w")
#   for line in open(in_file):
#     parts = line.strip().split("\t")
#     # skip the header
#     if parts[0] == "tweet_id":
#         print("\t".join(parts), file=trainfile)
#         continue
#     proc_tweet = preprocess(parts[2])
#     print("\t".join([parts[0], parts[1], proc_tweet, parts[3], parts[4]]), file=trainfile)
#   trainfile.close()

# # preprocess the development set
# def preprocess_development(in_file, out_dir):
#   count = 0
#   devfile = open(join(out_dir, "dev.tsv"), "w")
#   for line in open(in_file):
#     parts = line.strip().split("\t")
#     # skip the header
#     if parts[0] == "tweet_id":
#         print("\t".join(parts), file=devfile)
#         continue
#     proc_tweet = preprocess(parts[2])
#     print("\t".join([parts[0], parts[1], proc_tweet, parts[3], parts[4]]), file=devfile)
#   devfile.close()  

# preprocess the test set
def preprocess_test(in_file, out_dir):
  count = 0
  testfile = open(join(out_dir, "test.tsv"), "w")
  for line in open(in_file):
    parts = line.strip().split("\t")
    # skip the header
    if parts[0] == "tweet_id":
        print("\t".join(parts), file=testfile)
        continue
    proc_tweet = preprocess(parts[2])
    print("\t".join([parts[0], parts[1], proc_tweet, parts[3], parts[4]]), file=testfile)
  testfile.close()

# create output directory to store the preprocessed data sets
resource_dir, processed_dir = "resources", join("resources", "processed")
for dir in [resource_dir, processed_dir]:
  if not exists(dir):
    mkdir(dir)

# define the path to the data sets
train_file = "data/covid_diagnosis_tweets_training.tsv"
# dev_file = "data/covid_diagnosis_tweets_development.tsv"
test_file = "data/covid_diagnosis_tweets_test.tsv"

# call the methods
preprocess_training(train_file, processed_dir)
# preprocess_development(dev_file, processed_dir)
preprocess_test(test_file, processed_dir)

"""# Preview the preprocessed tweets.

"""

!head resources/processed/train.tsv ; echo
!head resources/processed/dev.tsv ; echo
!head resources/processed/test.tsv
!wc -l resources/processed/*

"""# Train and evaluate the classifiers.
Some popular pretrained transformer models to experiment with include: BERT ("bert-base-uncased"), DistilBERT ("distilbert-base-uncased"), RoBERTa ("roberta-large"), and BERTweet ("vinai/bertweet-large"). 
"""

for embedding in ["digitalepidemiologylab/covid-twitter-bert-v2"]:    #digitalepidemiologylab/covid-twitter-bert-v2
  print("Training on", embedding)

  # 1a. define the column format indicating which columns contain the text and labels
  column_name_map = {2: "text", 4: "label"}

  # 1b. load the preprocessed training, development, and test sets
  corpus: Corpus = CSVClassificationCorpus(processed_dir,
                                          column_name_map,
                                          label_type="label",
                                          skip_header=True,
                                          delimiter='\t') 
  # 2. create the label dictionary
  label_dict = corpus.make_label_dictionary(label_type="label")

  # 3. initialize the transformer document embeddings
  document_embeddings = TransformerDocumentEmbeddings(embedding,
                                                      fine_tune=True,
                                                      layers="all")

  # 4. create the text classifier
  classifier = TextClassifier(document_embeddings,
                              label_dictionary=label_dict,
                              label_type="label",
                              loss_weights={"1": 3.0, "0":1.0} # for imbalanced class
                              )

  # 5. initialize the trainer
  trainer = ModelTrainer(classifier,
                        corpus)

  # 6. start the training
  trainer.train('model/'+embedding,
              learning_rate=1e-5,
              mini_batch_size=8,
              max_epochs=5,
              patience=2,
              optimizer=Adam,
              train_with_dev=False,
              save_final_model=False
              )

"""# Output files from training.
A subdirectory of `covid/` called `model/` includes subdirectories for each model used for training. Each of these subdirectories includes the model that achieved the best performance based on the development set during training ("best-model.pt"), the training log ("training.log"), and a few other files. 

"""

print(os.listdir("model"))
print(os.listdir("model/distilbert-base-uncased/"))

"""# Make predictions on the test set or unlabeled data.

## Load the classification model.
"""

classifier = TextClassifier.load('model/roberta-large/best-model.pt')

"""## Predict labels."""

def predict_on_file(in_file, out_file):
  with open(out_file, "w") as ofile:
    for line in open(in_file):
      parts = line.strip().split("\t")
      tid, user_id, text, created_at, act_label = parts[0], parts[1], parts[2], parts[3], parts[4]
      if tid == "tweet_id":
        print("\t".join([tid, "user_id", "text", "created_at", "actual_label", "predicted_label"]), file=ofile)
        continue
      sentence = Sentence(preprocess(text))
      classifier.predict(sentence)
      label_obj = sentence.to_dict()['all labels'][0]
      pred_label, score = label_obj["value"], str(round(label_obj["confidence"], 2))
      output = "\t".join([tid, user_id, text, created_at, act_label, pred_label])
      print(output, file=ofile)

# make predictions
predict_on_file("data/covid_diagnosis_tweets_test.tsv", "data/covid_diagnosis_tweets_test_predictions.tsv")